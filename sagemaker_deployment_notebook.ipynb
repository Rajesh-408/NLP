{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53039626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5184d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "MODEL_LOCATION='sydney_category_model'\n",
    "\n",
    "JSON_LOCATION=''\n",
    "WEIGHTS_LOCATION=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14aa9c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from MODEL_LOCATION\n"
     ]
    }
   ],
   "source": [
    "#tf.enable_eager_execution\n",
    "#import tensorflow as tf\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "if MODEL_LOCATION!='':\n",
    "    from keras.models import load_model\n",
    "    model=load_model(MODEL_LOCATION)\n",
    "    print(\"loaded model from MODEL_LOCATION\")\n",
    "elif JSON_LOCATION!='':\n",
    "    from keras.models import model_from_json\n",
    "    json_file=open(JSON_LOCATION,'r')\n",
    "    loaded_model_json=json_file.read()\n",
    "    json_file.close()\n",
    "    model=model_from_json(loaded_model_json)\n",
    "    model.load_weights(WEIGHTS_LOCATION)\n",
    "    print(\"loaded model from JSON_LOCATION AND WEIGHTS_LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f05686a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Export directory export/Servo/1 already exists, and isn't empty. Please choose a different export directory, or delete all the contents of the specified directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m export_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport/Servo/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model_version\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Build the Protocol Buffer SavedModel at path defined by export_dir variable\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m builder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSavedModelBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSession(graph\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mget_default_graph()) \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Create prediction signature to be used by TensorFlow Serving Predict API\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     signature \u001b[38;5;241m=\u001b[39m predict_signature_def(inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39minput}, outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39moutput})\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/saved_model/builder_impl.py:448\u001b[0m, in \u001b[0;36mSavedModelBuilder.__init__\u001b[0;34m(self, export_dir)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, export_dir):\n\u001b[0;32m--> 448\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSavedModelBuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/saved_model/builder_impl.py:102\u001b[0m, in \u001b[0;36m_SavedModelBuilder.__init__\u001b[0;34m(self, export_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_io\u001b[38;5;241m.\u001b[39mfile_exists(export_dir):\n\u001b[1;32m    101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m file_io\u001b[38;5;241m.\u001b[39mlist_directory(export_dir):\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExport directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists, and isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt empty. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose a different export directory, or delete all the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents of the specified directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   file_io\u001b[38;5;241m.\u001b[39mrecursive_create_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_dir)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Export directory export/Servo/1 already exists, and isn't empty. Please choose a different export directory, or delete all the contents of the specified directory."
     ]
    }
   ],
   "source": [
    "#from keras import backend as K\n",
    "#K.set_learning_phase(0)\n",
    "from tensorflow.python.saved_model import builder\n",
    "from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.backend as K\n",
    "# Disable eager execution\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#with tf.compat.v1.Graph().as_default(): \n",
    "\n",
    "# Note: This directory structure will need to be followed \n",
    "model_version = '1'\n",
    "export_dir = 'export/Servo/' + model_version\n",
    "\n",
    "# Build the Protocol Buffer SavedModel at path defined by export_dir variable\n",
    "builder = builder.SavedModelBuilder(export_dir)\n",
    "\n",
    "with tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph()) as sess:\n",
    "\n",
    "# Create prediction signature to be used by TensorFlow Serving Predict API\n",
    "    signature = predict_signature_def(inputs={\"inputs\": model.input}, outputs={\"score\": model.output})\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess=sess, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2358878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['inputs'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 768)\r\n",
      "        name: flatten_input\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['score'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 7)\r\n",
      "        name: dense_2/Softmax:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n",
      "The MetaGraph with tag set ['serve'] contains the following ops: set()\r\n"
     ]
    }
   ],
   "source": [
    "model_path = 'export_10/Servo/1/'\n",
    "!saved_model_cli show --all --dir {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "868c93cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-993514063544/model/model.tar.gz'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "model_archive = 'model.tar.gz'\n",
    "with tarfile.open(model_archive, mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True) \n",
    "\n",
    "import sagemaker\n",
    "sess=sagemaker.Session()\n",
    "# upload model artifacts to S3\n",
    "model_data = sess.upload_data(path=model_archive, key_prefix='model')\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e3adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session=sagemaker.Session()\n",
    "#inputs = sagemaker_session.upload_data(path='model.tar.gz',key_prefix='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb823f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-2-993514063544'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Bucket name is\")\n",
    "sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f055382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8cc3eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-993514063544/model/model.tar.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz'\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aecfd124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role=get_execution_role()\n",
    "\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',\n",
    "                                 role=role,\n",
    "                                 framework_version = '2.13.0',\n",
    "                                 entry_point = 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dac67207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint sydneycategorymapping-latest-test: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/tensorflow/model.py:345\u001b[0m, in \u001b[0;36mTensorFlowModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TensorFlow version \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support EIA.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework_version\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTensorFlowModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_instance_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvolume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_recommendation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_recommendation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/model.py:1430\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1428\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1430\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1442\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:4727\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[1;32m   4724\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   4725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 4727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\n\u001b[1;32m   4729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:4072\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   4068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   4069\u001b[0m     EndpointName\u001b[38;5;241m=\u001b[39mendpoint_name, EndpointConfigName\u001b[38;5;241m=\u001b[39mconfig_name, Tags\u001b[38;5;241m=\u001b[39mtags\n\u001b[1;32m   4070\u001b[0m )\n\u001b[1;32m   4071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4072\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:4424\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   4418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   4419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   4420\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   4421\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   4422\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   4423\u001b[0m         )\n\u001b[0;32m-> 4424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   4425\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   4426\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   4427\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   4428\u001b[0m     )\n\u001b[1;32m   4429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint sydneycategorymapping-latest-test: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "health_check_timeout = 300\n",
    "# from tensorflow.python.client import session\n",
    "\n",
    "# with session.Session(graph=sagemaker_model.graph) as sess:\n",
    "sagemaker_predictor = sagemaker_model.deploy(initial_instance_count=1,\n",
    "                                  instance_type = 'ml.g4dn.4xlarge',\n",
    "                                  endpoint_name='sydneycategorymapping-latest-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab25bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting the text to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e88fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "#from transformers import BertTokenizer, BertModel\n",
    "tokenizer= BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e804a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def tokenize_bert_embeddings(texts):\n",
    "    inputs=tokenizer(texts,max_length=40,truncation=True,padding=\"max_length\",return_tensors=\"tf\")\n",
    "    outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    embeddings= tf.reduce_mean(embeddings,axis=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76eb32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"'alcohol/substance - inpatient professional alcohol/substance abuse'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b1be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tokenize_bert_embeddings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325b4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors= tf.convert_to_tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e02e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22388a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "#from sagemaker.tensorflow import NumpySerializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "sagemaker_session = sagemaker.Session()\n",
    "endpoint_name=\"sydneycategorymapping-latest\"\n",
    "\n",
    "predictor = sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name,sagemaker_session)#,serializer=JSONSerializer())#,sagemaker_session)\n",
    "#predictor_1 = sagemaker.predictor.Predictor(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09066be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.tensorflow.model.TensorFlowPredictor at 0x7ff7483dbca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ffebd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtensors\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensors' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = tensors.numpy()\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e7099bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa89c66c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{    \"error\": \"Session was not created with a graph before Run()!\"}\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/sydneycategorymapping-latest in account 993514063544 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/tensorflow/model.py:114\u001b[0m, in \u001b[0;36mTensorFlowPredictor.predict\u001b[0;34m(self, data, initial_args)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomAttributes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_attributes\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTensorFlowPredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/base_predictor.py:185\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m        as is.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m request_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_request_args(\n\u001b[1;32m    178\u001b[0m     data,\n\u001b[1;32m    179\u001b[0m     initial_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     custom_attributes,\n\u001b[1;32m    184\u001b[0m )\n\u001b[0;32m--> 185\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{    \"error\": \"Session was not created with a graph before Run()!\"}\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/sydneycategorymapping-latest in account 993514063544 for more information."
     ]
    }
   ],
   "source": [
    "predictor.predict([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021088bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
