from pymongo import MongoClient
import multiprocessing

# Define the processing function
def process_documents(documents):
    for document in documents:
        # Process the document here
        # Replace this with your actual processing logic
        print(document)

# Establish a connection to the MongoDB instance
client = MongoClient('mongodb://localhost:27017/')

# Replace 'your_db_name' and 'your_collection_name' with actual names
db = client['your_db_name']
collection = db['your_collection_name']

batch_size = 1000

# Create a cursor with batch size
cursor = collection.find().batch_size(batch_size)

# Create a list to store batches
batches = []

# Fetch and store batches
while cursor.alive:
    batch_documents = []
    for _ in range(batch_size):
        try:
            document = cursor.next()
            batch_documents.append(document)
        except StopIteration:
            break
    
    if batch_documents:
        batches.append(batch_documents)

# Close the cursor and MongoDB connection
cursor.close()
client.close()

# Process batches in parallel
with multiprocessing.Pool() as pool:
    pool.map(process_documents, batches)

# Calculate and print the total number of processed documents
total_processed = sum(len(batch) for batch in batches)
print(f"Total documents processed: {total_processed}")

print("All documents processed.")
