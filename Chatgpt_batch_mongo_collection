from pymongo import MongoClient
import multiprocessing
import time

# Helper function to recursively convert dictionaries to tuples
def convert_to_hashable(item):
    if isinstance(item, dict):
        return tuple((key, convert_to_hashable(value)) for key, value in item.items())
    if isinstance(item, list):
        return tuple(convert_to_hashable(value) for value in item)
    return item

# Define the processing function
def process_batch(batch_documents):
    unique_values_batch = set()
    for document in batch_documents:
        for value in document.values():
            hashable_value = convert_to_hashable(value)
            unique_values_batch.add(hashable_value)
    return unique_values_batch

# Establish a connection to the MongoDB instance
client = MongoClient('mongodb://localhost:27017/')

# Replace 'your_db_name' and 'your_collection_name' with actual names
db = client['your_db_name']
collection = db['your_collection_name']

batch_size = 1000
total_documents = collection.count_documents({})
total_batches = (total_documents + batch_size - 1) // batch_size

# Initialize a set to store unique hashable values
unique_values = set()

# Process batches using parallel processing
with multiprocessing.Pool() as pool:
    for batch_number in range(total_batches):
        batch_documents = collection.find().skip(batch_number * batch_size).limit(batch_size)
        batch_unique_values = pool.apply(process_batch, args=(batch_documents,))
        unique_values.update(batch_unique_values)

# Close the MongoDB connection
client.close()

# Print the unique values
for value in unique_values:
    print(value)

# Calculate and print processing time
end_time = time.time()
processing_time = end_time - start_time
print(f"Processing time: {processing_time:.2f} seconds")
