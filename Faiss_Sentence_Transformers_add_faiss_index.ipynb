{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b60143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cffeadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 10437684093684072020\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 2926942619\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 8437519852791075006\n",
       " physical_device_desc: \"device: 0, name: Quadro P620, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d4e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nls_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "stop_words=stopwords.words('english')\n",
    "punctuation=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79826c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_custom_embeddings(model_path,corpus_embeddings):\n",
    "    #store sentences & embeddings on disc\n",
    "    with open(model_path+'\\\\'+'embeddings.pkl',\"wb\") as fout:\n",
    "        pickle.dump({'Sentences':corpus, 'embeddings': corpus_embeddings},fout)\n",
    "    print(\"saved Custom embeddings\")\n",
    "\n",
    "def load_custom_embeddings(model_path):\n",
    "    with open(model_path+'/embeddings.pkl',\"rb\") as fin:\n",
    "        stored_data = pickle.load(fin)\n",
    "        stored_sentences = stored_data['Sentences']\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "    return stored_sentences,stored_embeddings\n",
    "\n",
    "def get_embeddings(sentence):\n",
    "    #encode sentence to get sentence embeddings\n",
    "    sentence_embedding=model.encode(sentence, convert_to_tensor=True)\n",
    "    return sentence_embedding\n",
    "\n",
    "def sentence_similarity_scores(sentence_embedding,\n",
    "                              custom_embeddings,\n",
    "                              stored_sentences,\n",
    "                              top_k,\n",
    "                              input_sentence):\n",
    "    #computing similarity scores with the corpus\n",
    "    cos_scores= util.pytorch_cos_sim(sentence_embedding, custom_embeddings)[0]\n",
    "    #sort the results in decreasing order and get the first top_k\n",
    "    top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "    print(\"sentence :\", input_sentence, \"\\n\")\n",
    "    print(\"Top\", top_k, \"most similar sentences in corpus\")\n",
    "    results={}\n",
    "    for idx in top_results[0:top_k]:\n",
    "        print(stored_sentences[idx],\"(scores:%4f)\" % (cos_scores[idx]))\n",
    "        results[f\"sentence{idx}\"]= ({\"predicted_sentence\": stored_sentences[idx],\"Scores\" : float(cos_scores[idx])})\n",
    "    return results\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert the text to title case\n",
    "    text = str(text).title()\n",
    "    # Remove the punctuation\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    # Remove the stop words\n",
    "    tokens = [token for token in text.split() if token.lower() not in stop_words]\n",
    "    # Convert the tokens back to a string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def concate_column_text(data):\n",
    "    df[\"concated_text\"]=df[[\"Category Name\",\"Service name\",\"Service Classification\"]].astype(str).agg(' '.join,axis=1)\n",
    "    return df[\"concated_text\"]\n",
    "\n",
    "\n",
    "def convert_column_to_list(data):\n",
    "    data=data.tolist()\n",
    "    return data\n",
    "\n",
    "def convert_df_to_list(data):\n",
    "    all_data=[]\n",
    "    corpus=[]\n",
    "    for values in df.columns:\n",
    "        listin=df[values].tolist()\n",
    "        all_data.append(listin)\n",
    "    complete_data = [element for innerList in all_data for element in innerList]\n",
    "    for word in complete_data:\n",
    "        if word not in corpus:\n",
    "            corpus.append(word)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2686b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the Excel files\n",
    "path_to_files = r\"C:\\Users\\AL44096\\Documents\\NLS_excel_files\"\n",
    "\n",
    "# Define the names of the Excel files\n",
    "file_names = ['National_Inc_Exc_Classification.xlsx'\n",
    "              ]\n",
    "\n",
    "# Define the name of the sheet in each Excel file that contains the text data\n",
    "#sheet_name = 'Sheet1'\n",
    "\n",
    "# Load and clean the text data from each Excel file\n",
    "cleaned_data = []\n",
    "for file_name in file_names:\n",
    "    # Load the data from the Excel file into a Pandas DataFrame\n",
    "    df = pd.read_excel(f'{path_to_files}/{file_name}')\n",
    "    data=concate_column_text(df)\n",
    "    #cleaned_data.append(data)\n",
    "    # Extract the relevant text data from the DataFrame\n",
    "    text_data = convert_column_to_list(data)\n",
    "    # Clean the text data\n",
    "    cleaned_text_data = [clean_text(text) for text in text_data]\n",
    "    cleaned_data.append(cleaned_text_data)\n",
    "corpus = [element for innerList in cleaned_data for element in innerList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7139a7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42097 entries, 0 to 42096\n",
      "Data columns (total 6 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   Category Name             42097 non-null  object\n",
      " 1   Service name              42097 non-null  object\n",
      " 2   Inclusion/Exclusion Name  42097 non-null  object\n",
      " 3   Type                      42097 non-null  object\n",
      " 4   Service Classification    42097 non-null  object\n",
      " 5   concated_text             42097 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bfeca73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Category Name', 'Service name', 'Inclusion/Exclusion Name', 'Type', 'Service Classification', 'concated_text'],\n",
       "    num_rows: 42097\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec15d748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABA Therapy ABA Therapy Inpatient Professional Applied Behavioral Analysis'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset['concated_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd4e373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb6cef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"concated_text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3011a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"concated_text\"]).numpy()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26833c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Category Name', 'Service name', 'Inclusion/Exclusion Name', 'Type', 'Service Classification', 'concated_text', 'embeddings'],\n",
       "    num_rows: 42097\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f471b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 161.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Category Name', 'Service name', 'Inclusion/Exclusion Name', 'Type', 'Service Classification', 'concated_text', 'embeddings'],\n",
       "    num_rows: 42097\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f29d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = \"WheelChair\"\n",
    "question_embedding = get_embeddings([search_term]).numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea08c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd9347d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01bb4725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.7771551609039307\n",
      "Service name: Durable Medical Equipment\n",
      "SCORE: 0.6895759105682373\n",
      "Service name: Medical Supply - Base\n",
      "SCORE: 0.6782286167144775\n",
      "Service name: Medical Supply - Major Medical\n",
      "SCORE: 0.5872913599014282\n",
      "Service name: Ground Ambulance\n",
      "SCORE: 0.5733773708343506\n",
      "Service name: Air Ambulance\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    #print(row)\n",
    "    #print(f\"concated_text: {row.concated_text}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"Service name: {row['Service name']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nls_gpu",
   "language": "python",
   "name": "nls_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
