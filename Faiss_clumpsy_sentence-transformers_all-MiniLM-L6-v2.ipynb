{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5694e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7b7a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8224388292900854182\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 2926942619\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 8525471715649172795\n",
       " physical_device_desc: \"device: 0, name: Quadro P620, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f50146e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\AL44096'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8b20bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nls_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "stop_words=stopwords.words('english')\n",
    "punctuation=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccf1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_custom_embeddings(model_path,corpus_embeddings):\n",
    "    #store sentences & embeddings on disc\n",
    "    with open(model_path+'\\\\'+'embeddings.pkl',\"wb\") as fout:\n",
    "        pickle.dump({'Sentences':corpus, 'embeddings': corpus_embeddings},fout)\n",
    "    print(\"saved Custom embeddings\")\n",
    "\n",
    "def load_custom_embeddings(model_path):\n",
    "    with open(model_path+'/embeddings.pkl',\"rb\") as fin:\n",
    "        stored_data = pickle.load(fin)\n",
    "        stored_sentences = stored_data['Sentences']\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "    return stored_sentences,stored_embeddings\n",
    "\n",
    "def encode_sentence(model,sentence):\n",
    "    #encode sentence to get sentence embeddings\n",
    "    sentence_embedding=model.encode(sentence, convert_to_tensor=True)\n",
    "    return sentence_embedding\n",
    "\n",
    "def sentence_similarity_scores(sentence_embedding,\n",
    "                              custom_embeddings,\n",
    "                              stored_sentences,\n",
    "                              top_k,\n",
    "                              input_sentence):\n",
    "    #computing similarity scores with the corpus\n",
    "    cos_scores= util.pytorch_cos_sim(sentence_embedding, custom_embeddings)[0]\n",
    "    #sort the results in decreasing order and get the first top_k\n",
    "    top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "    print(\"sentence :\", input_sentence, \"\\n\")\n",
    "    print(\"Top\", top_k, \"most similar sentences in corpus\")\n",
    "    results={}\n",
    "    for idx in top_results[0:top_k]:\n",
    "        print(stored_sentences[idx],\"(scores:%4f)\" % (cos_scores[idx]))\n",
    "        results[f\"sentence{idx}\"]= ({\"predicted_sentence\": stored_sentences[idx],\"Scores\" : float(cos_scores[idx])})\n",
    "    return results\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert the text to title case\n",
    "    text = str(text).title()\n",
    "    # Remove the punctuation\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    # Remove the stop words\n",
    "    tokens = [token for token in text.split() if token.lower() not in stop_words]\n",
    "    # Convert the tokens back to a string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def concate_column_text(data):\n",
    "    df[\"concated_text\"]=df[[\"Category Name\",\"Service name\",\"Service Classification\"]].astype(str).agg(' '.join,axis=1)\n",
    "    return df[\"concated_text\"]\n",
    "\n",
    "\n",
    "def convert_column_to_list(data):\n",
    "    data=data.tolist()\n",
    "    return data\n",
    "\n",
    "def convert_df_to_list(data):\n",
    "    all_data=[]\n",
    "    corpus=[]\n",
    "    for values in df.columns:\n",
    "        listin=df[values].tolist()\n",
    "        all_data.append(listin)\n",
    "    complete_data = [element for innerList in all_data for element in innerList]\n",
    "    for word in complete_data:\n",
    "        if word not in corpus:\n",
    "            corpus.append(word)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77528f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the Excel files\n",
    "path_to_files = r\"C:\\Users\\AL44096\\Documents\\NLS_excel_files\"\n",
    "\n",
    "# Define the names of the Excel files\n",
    "file_names = ['National_Inc_Exc_Classification.xlsx'\n",
    "              ]\n",
    "\n",
    "# Define the name of the sheet in each Excel file that contains the text data\n",
    "#sheet_name = 'Sheet1'\n",
    "\n",
    "# Load and clean the text data from each Excel file\n",
    "cleaned_data = []\n",
    "for file_name in file_names:\n",
    "    # Load the data from the Excel file into a Pandas DataFrame\n",
    "    df = pd.read_excel(f'{path_to_files}/{file_name}')\n",
    "    data=concate_column_text(df)\n",
    "    #cleaned_data.append(data)\n",
    "    # Extract the relevant text data from the DataFrame\n",
    "    text_data = convert_column_to_list(data)\n",
    "    # Clean the text data\n",
    "    cleaned_text_data = [clean_text(text) for text in text_data]\n",
    "    cleaned_data.append(cleaned_text_data)\n",
    "corpus = [element for innerList in cleaned_data for element in innerList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9388621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the Excel files\n",
    "path_to_files = r\"C:\\Users\\AL44096\\Documents\\NLS_excel_files\"\n",
    "\n",
    "# Define the names of the Excel files\n",
    "file_names = ['National_Inc_Exc_Classification.xlsx'\n",
    "              ]\n",
    "\n",
    "# Define the name of the sheet in each Excel file that contains the text data\n",
    "#sheet_name = 'Sheet1'\n",
    "\n",
    "# Load and clean the text data from each Excel file\n",
    "cleaned_data = []\n",
    "for file_name in file_names:\n",
    "    # Load the data from the Excel file into a Pandas DataFrame\n",
    "    df = pd.read_excel(f'{path_to_files}/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f30754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/AL44096/.cache/huggingface/datasets/lewtun___json/lewtun--github-issues-cff5093ecc410ea2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8b6a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c0dc1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\AL44096\\.cache\\huggingface\\datasets\\lewtun___json\\lewtun--github-issues-cff5093ecc410ea2\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-438a407e5582fde3.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c36727e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe08553",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1c7e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b787d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url   \n",
       "0  https://github.com/huggingface/datasets/issues...  \\\n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title   \n",
       "0                              Protect master branch  \\\n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments   \n",
       "0                    Cool, I think we can do both :)  \\\n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91caa687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be5d2bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d84302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6b00dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22b2f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb43adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6115acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(comments_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2650c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>64</td>\n",
       "      <td>Protect master branch \\n After accidental merg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>50</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>28</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Well it can cause issue with anyone that updat...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>22</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>I just merged a fix, let me know if you're sti...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>27</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url   \n",
       "0  https://github.com/huggingface/datasets/issues...  \\\n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title   \n",
       "0                              Protect master branch  \\\n",
       "1  Backwards compatibility broken for cached data...   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "4  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments   \n",
       "0  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...  \\\n",
       "1  Hi ! I guess the caching mechanism should have...   \n",
       "2  If it's easy enough to implement, then yes ple...   \n",
       "3  Well it can cause issue with anyone that updat...   \n",
       "4  I just merged a fix, let me know if you're sti...   \n",
       "\n",
       "                                                body  comment_length   \n",
       "0  After accidental merge commit (91c55355b634d0d...              64  \\\n",
       "1  ## Describe the bug\\r\\nAfter upgrading to data...              50   \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...              28   \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...              22   \n",
       "4  ## Describe the bug\\r\\nAfter upgrading to data...              27   \n",
       "\n",
       "                                                text  \n",
       "0  Protect master branch \\n After accidental merg...  \n",
       "1  Backwards compatibility broken for cached data...  \n",
       "2  Backwards compatibility broken for cached data...  \n",
       "3  Backwards compatibility broken for cached data...  \n",
       "4  Backwards compatibility broken for cached data...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4385cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMPNetModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1645ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a56fea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"tf\"\n",
    "    )\n",
    "    encoded_input = {k: v for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83ffc046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c94831",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a306a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee5178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset['embeddings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feed3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(embeddings_dataset)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24534430",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7291a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f79f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a06086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ee130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd10e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c80484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d497846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dcc47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d821f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc57a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings =  model.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6eaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_path= r\"C:\\Users\\AL44096\\Documents\\sentence_model\"\n",
    "save_custom_embeddings(model_path,corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_sentences,stored_embeddings=load_custom_embeddings(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf947e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=stored_embeddings.numpy()[0]\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820deb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86141bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.add_faiss_index()#column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0944c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cfb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcb9c053",
   "metadata": {},
   "source": [
    "# searching through faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = stored_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196238eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(stored_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03530641",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ffadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "xq = model.encode([\"WheelChair\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb764e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{i}: {corpus[i]}' for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da43c4",
   "metadata": {},
   "source": [
    "# searching by creating an clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 50  # how many cells\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.train(stored_embeddings)\n",
    "index.is_trained # check if index is now trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(stored_embeddings)\n",
    "index.ntotal  # number of embeddings indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c83522",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{i}: {corpus[i]}' for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d72db1",
   "metadata": {},
   "source": [
    "# Searching by all the 10 nearest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6257fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.nprobe = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21791725",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{i}: {corpus[i]}' for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9723ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[corpus[i] for i in I[0]]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Below are some quick examples.\n",
    "# Extract column values by using DataFrame.loc[] property.\n",
    "df2=df.loc[df['Fee'] == 30000, 'Courses']\n",
    "\n",
    "# To get First Element by using .iloc[] method.\n",
    "df2=df.loc[df['Fee'] == 30000, 'Courses'].iloc[0]\n",
    "\n",
    "# Extract column values by DataFrame.item() method\n",
    "df2=df.loc[df['Fee'] == 30000, 'Courses'].item()\n",
    "\n",
    "# Using DataFrame.query() method extract column values.\n",
    "df2=df.query('Fee==25000')['Courses']\n",
    "\n",
    "# Using DataFrame.values() property.\n",
    "df2=df[df['Fee']==22000]['Courses'].values[0]\n",
    "\n",
    "# Other example.\n",
    "df2=df[df['Fee']==22000]['Courses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in p:\n",
    "    y=df.loc[df[\"concated_text\"]==i]['Service name']#.values[0]#item()#iloc[0]\n",
    "    print(y)#.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[11394]['Service name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e7b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3c45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nls_gpu",
   "language": "python",
   "name": "nls_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
